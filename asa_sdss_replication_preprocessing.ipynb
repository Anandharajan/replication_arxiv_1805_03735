{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Aggregation Rules for Anomaly Detection in Computer Network Traffic\n",
    "## Replication File 1 of 3\n",
    "\n",
    "Benjamin J. Radford, Bartley D. Richardson, and Shawn E. Davis\n",
    "\n",
    "Paper available: [arXiv:1805.03735v2](https://arxiv.org/abs/1805.03735).\n",
    "\n",
    "DISTRIBUTION STATEMENT A: Approved for public release. \n",
    "\n",
    "This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import dependencies\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.engine.topology import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.optimizers import TFOptimizer, RMSprop\n",
    "\n",
    "## Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set modeling parameters\n",
    "##\n",
    "\n",
    "seq_len = 10\n",
    "seq_skip = 3\n",
    "\n",
    "w2v_size = 25\n",
    "w2v_min_count = 3\n",
    "w2v_window = 10\n",
    "w2v_workers = 4\n",
    "\n",
    "embedding_a_size = 100\n",
    "lstm_a_size = 25\n",
    "lstm_b_size = 25\n",
    "dense_size = 100\n",
    "\n",
    "validation_split = 0.1\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "\n",
    "cicids_training = datetime.datetime.strptime(\"2017-07-04 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "num_models = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CICIDS2017 data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/anaconda3/envs/py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (20,21,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/anaconda3/envs/py2/lib/python2.7/site-packages/ipykernel_launcher.py:15: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Read in CICIDS2017 Dataset\n",
    "##\n",
    "\n",
    "## Set a working directory and point to your data\n",
    "wd = \"./\"\n",
    "cicids_files = \"data/CICIDS2017/*.csv\"\n",
    "\n",
    "## Load the data\n",
    "print(\"Reading CICIDS2017 data...\")\n",
    "files = glob.glob(wd+cicids_files)\n",
    "cicids_data = []\n",
    "for ff in files:\n",
    "    cicids_data.append(pandas.read_csv(ff, encoding=\"Latin1\"))\n",
    "cicids_data = pandas.concat(cicids_data)\n",
    "\n",
    "## Set CICIDS2017 internal IPs in case they're required later\n",
    "cicids_internal = set([\"192.168.10.50\", \"205.174.165.68\",\n",
    "            \"192.168.10.51\", \"205.174.165.66\", \"192.168.10.19\",\n",
    "            \"192.168.10.17\", \"192.168.10.16\",\n",
    "            \"192.168.10.12\", \"192.168.10.9\",\n",
    "            \"192.168.10.5\", \"192.168.10.8\",\n",
    "            \"192.168.10.14\", \"192.168.10.15\",\n",
    "            \"192.168.10.25\", \"205.174.165.80\", \n",
    "            \"172.16.0.1\",\"192.168.10.3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Define functions required to clean CICIDS2017\n",
    "##\n",
    "\n",
    "def cicids_fixdate(x):\n",
    "    \"\"\"Fix dates - This is specific to the CICIDS2017 dataset.\"\"\"\n",
    "    try:\n",
    "        d = datetime.datetime.strptime(x,\"%d/%m/%Y %H:%M:%S\")\n",
    "    except:\n",
    "        d = datetime.datetime.strptime(x,\"%d/%m/%Y %H:%M\")\n",
    "    return(d)\n",
    "\n",
    "def cicids_fixattacknames(x):\n",
    "    \"\"\"Fix attack names - This is specific to CICIDS2017 dataset.\"\"\"\n",
    "    return(re.sub('[^0-9a-zA-Z ]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing column names...\n",
      "BENIGN    713828\n",
      "DDoS       41835\n",
      "Name: Label, dtype: int64\n",
      "Dropping incomplete records...\n",
      "Fixing the timestamps...\n",
      "Fixing attack names...\n",
      "Generating floor(log2(bytes)) feature...\n",
      "Generate unique index values...\n",
      "Removing entries without an internal machine...\n",
      "Before remove ext-ext: 755663\n",
      "After remove ext-ext: 755384\n",
      "Generate internal and external host fields...\n",
      "Generating dyads...\n",
      "Generating hour bins...\n",
      "Generating protocol:port:floor(log(avg(bytes/packet))) feature (a.k.a. 'token_source' and 'token_destination')...\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Clean CICIDS2017\n",
    "## \n",
    "\n",
    "print(\"Fixing column names...\")\n",
    "cicids_data.columns = [a.lstrip() for a in cicids_data.columns]\n",
    "cicids_data = cicids_data[[\"Source Port\",\"Destination Port\",\"Source IP\",\"Destination IP\",\"Label\",\"Timestamp\",\"Protocol\",\"Average Packet Size\",\"Total Fwd Packets\",\"Total Backward Packets\"]]\n",
    "print(cicids_data.Label.value_counts())\n",
    "\n",
    "print(\"Dropping incomplete records...\")\n",
    "cicids_data = cicids_data.dropna(axis=0,how=\"all\")\n",
    "\n",
    "print(\"Fixing the timestamps...\")\n",
    "cicids_data[\"Timestamp\"] = cicids_data[\"Timestamp\"].apply(lambda x: cicids_fixdate(x))\n",
    "\n",
    "print(\"Fixing attack names...\")\n",
    "cicids_data[\"Label\"] = cicids_data[\"Label\"].apply(lambda x: cicids_fixattacknames(x))\n",
    "\n",
    "print(\"Generating floor(log2(bytes)) feature...\")\n",
    "cicids_data[\"logbytes\"] = cicids_data[[\"Average Packet Size\",\"Total Fwd Packets\",\"Total Backward Packets\"]].apply(lambda x: math.floor(math.log(float(x[0] * (x[1]+x[2]))+1.,2)),axis=1)\n",
    "\n",
    "print(\"Generate unique index values...\")\n",
    "cicids_data[\"index\"] = list(range(0,cicids_data.shape[0]))\n",
    "\n",
    "print(\"Removing entries without an internal machine...\")\n",
    "print(\"Before remove ext-ext: {}\".format(str(cicids_data.shape[0])))\n",
    "cicids_data = cicids_data.loc[(cicids_data[\"Source IP\"].isin(cicids_internal) | cicids_data[\"Destination IP\"].isin(cicids_internal))]\n",
    "print(\"After remove ext-ext: {}\".format(str(cicids_data.shape[0])))\n",
    "\n",
    "print(\"Generate internal and external host fields...\")\n",
    "cicids_data[\"internal\"] = cicids_data[[\"Source IP\",\"Destination IP\"]].apply(lambda x: x[0] if x[0] in cicids_internal else x[1], axis=1)\n",
    "cicids_data[\"external\"] = cicids_data[[\"Source IP\",\"Destination IP\"]].apply(lambda x: x[1] if x[0] in cicids_internal else x[0], axis=1)\n",
    "\n",
    "print(\"Generating dyads...\")\n",
    "cicids_data[\"dyad\"] = cicids_data[[\"Source IP\",\"Destination IP\"]].apply(lambda x: x[0]+\":\"+x[1], axis=1)\n",
    "\n",
    "print(\"Generating hour bins...\")\n",
    "cicids_data[\"hour\"] = cicids_data[\"Timestamp\"].apply(lambda x: str(x)[0:13])\n",
    "\n",
    "print(\"Generating protocol:port:floor(log(avg(bytes/packet))) feature (a.k.a. 'token_source' and 'token_destination')...\")\n",
    "cicids_data[\"protobytes\"] = cicids_data[[\"Protocol\",\"logbytes\"]].apply(lambda x: str(x[0])+\":\"+str(x[1]), axis=1)\n",
    "cicids_data[\"port\"] = cicids_data[[\"Source Port\",\"Destination Port\"]].apply(lambda x: \"port:\"+str(min(min(x[0],10000),min(x[1],10000))), axis=1)\n",
    "cicids_data[\"label\"] = cicids_data[\"Label\"].apply(lambda x: x)\n",
    "cicids_data[\"time\"] = cicids_data[\"Timestamp\"].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate naive token frequency models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate naive token frequency models...\")\n",
    "protobytes_train = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) < cicids_training].protobytes.tolist()\n",
    "protobytes_test = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) >= cicids_training].protobytes.tolist()\n",
    "ports_train = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) < cicids_training].port.tolist()\n",
    "ports_test = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) >= cicids_training].port.tolist()\n",
    "labels = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) >= cicids_training].label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "protobytes_freq = Counter(protobytes_train)\n",
    "ports_freq = Counter(ports_train)\n",
    "protobytes_sum = float(sum([a for b,a in protobytes_freq.iteritems()]))\n",
    "ports_sum = float(sum([a for b,a in ports_freq.iteritems()]))\n",
    "protobytes_prob = {w:(f/protobytes_sum) for w,f in protobytes_freq.iteritems()}\n",
    "ports_prob = {w:(f/ports_sum) for w,f in ports_freq.iteritems()}\n",
    "protobytes_scores = np.asarray([protobytes_prob[a] if a in protobytes_prob else 0. for a in protobytes_test])\n",
    "ports_scores = np.asarray([ports_prob[a] if a in ports_prob else 0. for a in ports_test])\n",
    "pickle.dump(np.array(labels),open(\"results/freq_labels.pickle\",\"wb\"))\n",
    "pickle.dump(ports_scores,open(\"results/freq_ports.pickle\",\"wb\"))\n",
    "pickle.dump(protobytes_scores,open(\"results/freq_protobytes.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating forward and reverse dictionaries...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating forward and reverse dictionaries...\")\n",
    "protobytes_fwd_dict = {str(w):str(i+1) for i, w in enumerate(list(set(cicids_data[\"protobytes\"].unique().tolist())))}\n",
    "protobytes_fwd_dict[\"0\"] = 0\n",
    "protobytes_rev_dict = {i:w for w,i in protobytes_fwd_dict.iteritems()}\n",
    "port_fwd_dict = {str(w):str(i+1) for i, w in enumerate(list(set(cicids_data[\"port\"].unique().tolist())))}\n",
    "port_fwd_dict[\"0\"] = 0\n",
    "port_rev_dict = {i:w for w,i in protobytes_fwd_dict.iteritems()}\n",
    "\n",
    "cicids_data = cicids_data.replace({\"protobytes\": protobytes_fwd_dict, \"port\": port_fwd_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat token strings...\n",
      "Generate all sequences...\n",
      "Add training set indicator...\n",
      "Prepend sequences with zero values...\n",
      "Split into training and test sets...\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Generate sequences from CICIDS2017\n",
    "##\n",
    "\n",
    "## Concat token strings\n",
    "print(\"Concat token strings...\")\n",
    "cicids_source_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"Source IP\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "cicids_destination_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"Destination IP\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "cicids_dyad_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"dyad\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "cicids_internal_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"internal\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "cicids_external_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"external\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "\n",
    "## Generate sequences\n",
    "print(\"Generate all sequences...\")\n",
    "cicids_source_hour[\"protobytes_sequence\"] = cicids_source_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_destination_hour[\"protobytes_sequence\"] = cicids_destination_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_dyad_hour[\"protobytes_sequence\"] = cicids_dyad_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_internal_hour[\"protobytes_sequence\"] = cicids_internal_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_external_hour[\"protobytes_sequence\"] = cicids_external_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "cicids_source_hour[\"port_sequence\"] = cicids_source_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_destination_hour[\"port_sequence\"] = cicids_destination_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_dyad_hour[\"port_sequence\"] = cicids_dyad_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_internal_hour[\"port_sequence\"] = cicids_internal_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_external_hour[\"port_sequence\"] = cicids_external_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "cicids_source_hour[\"label_sequence\"] = cicids_source_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_destination_hour[\"label_sequence\"] = cicids_destination_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_dyad_hour[\"label_sequence\"] = cicids_dyad_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_internal_hour[\"label_sequence\"] = cicids_internal_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_external_hour[\"label_sequence\"] = cicids_external_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "cicids_source_hour[\"time_sequence\"] = cicids_source_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_destination_hour[\"time_sequence\"] = cicids_destination_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_dyad_hour[\"time_sequence\"] = cicids_dyad_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_internal_hour[\"time_sequence\"] = cicids_internal_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_external_hour[\"time_sequence\"] = cicids_external_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "## Add training set indicator\n",
    "print(\"Add training set indicator...\")\n",
    "cicids_source_hour[\"training\"] = cicids_source_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "cicids_destination_hour[\"training\"] = cicids_destination_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "cicids_dyad_hour[\"training\"] = cicids_dyad_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "cicids_internal_hour[\"training\"] = cicids_internal_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "cicids_external_hour[\"training\"] = cicids_external_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "\n",
    "## Prepend sequences to correct length\n",
    "print(\"Prepend sequences with zero values...\")\n",
    "cicids_source_hour[\"port_sequence\"] = cicids_source_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_destination_hour[\"port_sequence\"] = cicids_destination_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_dyad_hour[\"port_sequence\"] = cicids_dyad_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_internal_hour[\"port_sequence\"] = cicids_internal_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_external_hour[\"port_sequence\"] = cicids_external_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "\n",
    "cicids_source_hour[\"protobytes_sequence\"] = cicids_source_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_destination_hour[\"protobytes_sequence\"] = cicids_destination_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_dyad_hour[\"protobytes_sequence\"] = cicids_dyad_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_internal_hour[\"protobytes_sequence\"] = cicids_internal_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_external_hour[\"protobytes_sequence\"] = cicids_external_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "\n",
    "cicids_source_hour[\"label_sequence\"] = cicids_source_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_destination_hour[\"label_sequence\"] = cicids_destination_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_dyad_hour[\"label_sequence\"] = cicids_dyad_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_internal_hour[\"label_sequence\"] = cicids_internal_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_external_hour[\"label_sequence\"] = cicids_external_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "\n",
    "cicids_source_hour[\"time_sequence\"] = cicids_source_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_destination_hour[\"time_sequence\"] = cicids_destination_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_dyad_hour[\"time_sequence\"] = cicids_dyad_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_internal_hour[\"time_sequence\"] = cicids_internal_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_external_hour[\"time_sequence\"] = cicids_external_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "\n",
    "## Split into training and test sets\n",
    "print(\"Split into training and test sets...\")\n",
    "cicids_source_hour_training = cicids_source_hour[cicids_source_hour[\"training\"]==True]\n",
    "cicids_destination_hour_training = cicids_destination_hour[cicids_destination_hour[\"training\"]==True]\n",
    "cicids_dyad_hour_training = cicids_dyad_hour[cicids_dyad_hour[\"training\"]==True]\n",
    "cicids_internal_hour_training = cicids_internal_hour[cicids_internal_hour[\"training\"]==True]\n",
    "cicids_external_hour_training = cicids_external_hour[cicids_external_hour[\"training\"]==True]\n",
    "\n",
    "cicids_source_hour_testing = cicids_source_hour[cicids_source_hour[\"training\"]==False]\n",
    "cicids_destination_hour_testing = cicids_destination_hour[cicids_destination_hour[\"training\"]==False]\n",
    "cicids_dyad_hour_testing = cicids_dyad_hour[cicids_dyad_hour[\"training\"]==False]\n",
    "cicids_internal_hour_testing = cicids_internal_hour[cicids_internal_hour[\"training\"]==False]\n",
    "cicids_external_hour_testing = cicids_external_hour[cicids_external_hour[\"training\"]==False]\n",
    "\n",
    "# def subToken(corpus, count):\n",
    "#     counts = Counter([a for sublist in corpus for a in sublist])\n",
    "#     above_count = set([key for key,val in counts.iteritems() if val >= count])\n",
    "#     return [map(lambda x: x if x in above_count else \"UNK\", a) for a in corpus]\n",
    "\n",
    "# ## Word2Vec models for sequence tokens\n",
    "# w2v_source_hour = Word2Vec(subToken(cicids_source_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n",
    "# w2v_destination_hour = Word2Vec(subToken(cicids_destination_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n",
    "# w2v_dyad_hour = Word2Vec(subToken(cicids_dyad_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n",
    "# w2v_internal_hour = Word2Vec(subToken(cicids_internal_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n",
    "# w2v_external_hour = Word2Vec(subToken(cicids_external_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save stuff...\n"
     ]
    }
   ],
   "source": [
    "print(\"Save stuff...\")\n",
    "pickle.dump(port_fwd_dict,open(\"data/port_fwd_dict.pickle\",\"wb\"))\n",
    "pickle.dump(port_rev_dict,open(\"data/port_rev_dict.pickle\",\"wb\"))\n",
    "pickle.dump(protobytes_fwd_dict,open(\"data/protobytes_fwd_dict.pickle\",\"wb\"))\n",
    "pickle.dump(protobytes_rev_dict,open(\"data/protobytes_rev_dict.pickle\",\"wb\"))\n",
    "cicids_source_hour_training.to_pickle(\"data/cicids_source_hour_training.pickle\")\n",
    "cicids_destination_hour_training.to_pickle(\"data/cicids_destination_hour_training.pickle\")\n",
    "cicids_dyad_hour_training.to_pickle(\"data/cicids_dyad_hour_training.pickle\")\n",
    "cicids_internal_hour_training.to_pickle(\"data/cicids_internal_hour_training.pickle\")\n",
    "cicids_external_hour_training.to_pickle(\"data/cicids_external_hour_training.pickle\")\n",
    "cicids_source_hour_testing.to_pickle(\"data/cicids_source_hour_testing.pickle\")\n",
    "cicids_destination_hour_testing.to_pickle(\"data/cicids_destination_hour_testing.pickle\")\n",
    "cicids_dyad_hour_testing.to_pickle(\"data/cicids_dyad_hour_testing.pickle\")\n",
    "cicids_internal_hour_testing.to_pickle(\"data/cicids_internal_hour_testing.pickle\")\n",
    "cicids_external_hour_testing.to_pickle(\"data/cicids_external_hour_testing.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_mode(X):\n",
    "    X = filter(lambda x: x != \"X\", X)\n",
    "    if len(set(X)) == 1 and list(set(X))[0] == \"BENIGN\":\n",
    "        return(\"BENIGN\")\n",
    "    else:\n",
    "        X = [a for a in X if a != \"BENIGN\"]\n",
    "        return(max(set(X), key=X.count))\n",
    "    \n",
    "def cicids_processing(sequences, labels, dict_size, seq_len, seq_skip, resample=False):\n",
    "    X = []\n",
    "    Y = []\n",
    "    L = []\n",
    "    for ii, token_seq in enumerate(sequences):\n",
    "        label_seq = labels[ii]\n",
    "        for jj in range(0, len(token_seq)-seq_len, seq_skip):\n",
    "            X.append(token_seq[jj:(jj+seq_len)])\n",
    "            Y.append(to_categorical(int(token_seq[jj+seq_len])-1, dict_size))\n",
    "            L.append(label_seq[jj+seq_len])\n",
    "            \n",
    "    if resample==True:\n",
    "        indices = np.random.choice(np.arange(len(X)),size=len(X),replace=True)\n",
    "    else:\n",
    "        indices = np.arange(len(X))\n",
    "    return((np.array(X)[indices], np.array(Y)[indices], np.array(L)[indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_2: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b058c9ce01ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0membedding_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport_fwd_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlstm_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_a_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdropout_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlstm_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_b_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/keras/layers/wrappers.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_2: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": [
    "# cicids_source_hour_testing = pickle.load(open(\"data/cicids_source_hour_testing.pickle\",\"rb\"))\n",
    "X_test, Y_test, L_test = cicids_processing(cicids_source_hour_testing[\"port_sequence\"].tolist(),\n",
    "                         cicids_source_hour_testing[\"label_sequence\"].tolist(),\n",
    "                         len(port_fwd_dict)-1, seq_len, seq_skip, False)\n",
    "\n",
    "pickle.dump([X_test, Y_test, L_test],open(\"results/source_ports_truth.pickle\",\"wb\"))\n",
    "\n",
    "for ii in range(num_models):\n",
    "    \n",
    "    X_train, Y_train, L_test = cicids_processing(cicids_source_hour_training[\"port_sequence\"].tolist(),\n",
    "                         cicids_source_hour_training[\"label_sequence\"].tolist(),\n",
    "                         len(port_fwd_dict)-1, seq_len, seq_skip, True)\n",
    "    \n",
    "    model_input = Input(shape=(seq_len, 1))\n",
    "    embedding_a = Embedding(len(port_fwd_dict), 100, mask_zero=True)(model_input)\n",
    "    lstm_a = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(embedding_a)\n",
    "    dropout_a = Dropout(0.2)(lstm_a)\n",
    "    lstm_b = Bidirectional(LSTM(lstm_b_size, return_sequences=False, activation=\"relu\"), merge_mode=\"concat\")(lstm_a)\n",
    "    dropout_b = Dropout(0.2)(lstm_b)\n",
    "    dense_layer = Dense(dense_size, activation=\"linear\")(dropout_b)\n",
    "    dropout_c = Dropout(0.2)(dense_layer)\n",
    "    lstm_c = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(dropout_c)\n",
    "    dropout_c = Dropout(0.2)(lstm_c)\n",
    "    model_output = Dense(len(port_fwd_dict)-1)(dropout_c)\n",
    "\n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    model.compile(optimizer=TFOptimizer(tf.contrib.opt.LazyAdamOptimizer()), loss='categorical_crossentropy')\n",
    "    print(model.summary())\n",
    "    \n",
    "#     model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "#     l2_in = Lambda(lambda x: K.l2_normalize(x, axis=2))(model_input)\n",
    "#     l2_out = Lambda(lambda x: K.l2_normalize(x, axis=2))(lstm_d)\n",
    "#     l2_multiply = Multiply()([l2_in,l2_out])\n",
    "#     dot_product = Lambda(lambda x: -K.sum(x,axis=2))(l2_multiply)\n",
    "#     output_max = Lambda(lambda x: K.max(x, axis=1, keepdims=True))(dot_product)\n",
    "#     output_avg = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(dot_product)\n",
    "    \n",
    "#     flat_in = Flatten()(model_input)\n",
    "#     flat_out = Flatten()(lstm_d)\n",
    "#     output_dot = Dot(axes=(1,1), normalize=True)([flat_in, flat_out])\n",
    "\n",
    "#     max_pred_model = Model(inputs=model_input, outputs=output_max)\n",
    "#     avg_pred_model = Model(inputs=model_input, outputs=output_avg)\n",
    "#     dot_pred_model = Model(inputs=model_input, outputs=output_dot)\n",
    "    \n",
    "#     source_max_preds = max_pred_model.predict(X_test, batch_size=batch_size)\n",
    "#     source_avg_preds = avg_pred_model.predict(X_test, batch_size=batch_size)\n",
    "#     source_dot_preds = dot_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    \n",
    "#     pickle.dump([source_max_preds, source_avg_preds, source_dot_preds],\n",
    "#                 open(\"asa_sdss_final/source_\"+str(ii)+\".pickle\",\"wb\"))\n",
    "    \n",
    "#     print(str(ii)+\" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_max_preds_list = []\n",
    "destination_avg_preds_list = []\n",
    "destination_dot_preds_list = []\n",
    "\n",
    "X_test, Y_test = cicids_processing(cicids_destination_hour_testing[\"sequence\"].tolist(),\n",
    "                         cicids_destination_hour_testing[\"label_sequence\"].tolist(),\n",
    "                         w2v_destination_hour, w2v_size, seq_len, seq_skip, False)\n",
    "pickle.dump([X_test, Y_test],open(\"asa_sdss_final/destination_truth.pickle\",\"wb\"))\n",
    "\n",
    "for ii in range(num_models):\n",
    "    \n",
    "    X_train, Y_train = cicids_processing(cicids_destination_hour_training[\"sequence\"].tolist(),\n",
    "                         cicids_destination_hour_training[\"label_sequence\"].tolist(),\n",
    "                         w2v_destination_hour, w2v_size, seq_len, seq_skip, True)\n",
    "    \n",
    "    model_input = Input(shape=(seq_len, w2v_size))\n",
    "    lstm_a = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(model_input)\n",
    "    lstm_b = Bidirectional(LSTM(lstm_b_size, return_sequences=False), merge_mode=\"concat\")(lstm_a)\n",
    "    dropout_a = Dropout(0.2)(lstm_b)\n",
    "    dense_layer = Dense(dense_size, activation=\"tanh\")(dropout_a)\n",
    "    repeat_layer = RepeatVector(seq_len)(dense_layer)\n",
    "    dropout_b = Dropout(0.2)(repeat_layer)\n",
    "    lstm_c = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(dropout_b)\n",
    "    dropout_c = Dropout(0.2)(lstm_c)\n",
    "    lstm_d = LSTM(w2v_size, return_sequences=True)(dropout_c)\n",
    "\n",
    "    model = Model(inputs=model_input, outputs=lstm_d)\n",
    "    model.compile(optimizer=\"RMSprop\", loss='cosine_proximity')\n",
    "    model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    l2_in = Lambda(lambda x: K.l2_normalize(x, axis=2))(model_input)\n",
    "    l2_out = Lambda(lambda x: K.l2_normalize(x, axis=2))(lstm_d)\n",
    "    l2_multiply = Multiply()([l2_in,l2_out])\n",
    "    dot_product = Lambda(lambda x: -K.sum(x,axis=2))(l2_multiply)\n",
    "    output_max = Lambda(lambda x: K.max(x, axis=1, keepdims=True))(dot_product)\n",
    "    output_avg = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(dot_product)\n",
    "    \n",
    "    flat_in = Flatten()(model_input)\n",
    "    flat_out = Flatten()(lstm_d)\n",
    "    output_dot = Dot(axes=(1,1), normalize=True)([flat_in, flat_out])\n",
    "\n",
    "    max_pred_model = Model(inputs=model_input, outputs=output_max)\n",
    "    avg_pred_model = Model(inputs=model_input, outputs=output_avg)\n",
    "    dot_pred_model = Model(inputs=model_input, outputs=output_dot)\n",
    "    \n",
    "    destination_max_preds = max_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    destination_avg_preds = avg_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    destination_dot_preds = dot_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    \n",
    "    pickle.dump([destination_max_preds, destination_avg_preds, destination_dot_preds],\n",
    "                open(\"asa_sdss_final/destination_\"+str(ii)+\".pickle\",\"wb\"))\n",
    "    \n",
    "    print(str(ii)+\" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyad_max_preds_list = []\n",
    "dyad_avg_preds_list = []\n",
    "dyad_dot_preds_list = []\n",
    "\n",
    "X_test, Y_test = cicids_processing(cicids_dyad_hour_testing[\"sequence\"].tolist(),\n",
    "                         cicids_dyad_hour_testing[\"label_sequence\"].tolist(),\n",
    "                         w2v_dyad_hour, w2v_size, seq_len, seq_skip, False)\n",
    "pickle.dump([X_test, Y_test],open(\"asa_sdss_final/dyad_truth.pickle\",\"wb\"))\n",
    "\n",
    "for ii in range(num_models):\n",
    "    \n",
    "    X_train, Y_train = cicids_processing(cicids_dyad_hour_training[\"sequence\"].tolist(),\n",
    "                         cicids_dyad_hour_training[\"label_sequence\"].tolist(),\n",
    "                         w2v_dyad_hour, w2v_size, seq_len, seq_skip, True)\n",
    "    \n",
    "    model_input = Input(shape=(seq_len, w2v_size))\n",
    "    lstm_a = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(model_input)\n",
    "    lstm_b = Bidirectional(LSTM(lstm_b_size, return_sequences=False), merge_mode=\"concat\")(lstm_a)\n",
    "    dropout_a = Dropout(0.2)(lstm_b)\n",
    "    dense_layer = Dense(dense_size, activation=\"tanh\")(dropout_a)\n",
    "    repeat_layer = RepeatVector(seq_len)(dense_layer)\n",
    "    dropout_b = Dropout(0.2)(repeat_layer)\n",
    "    lstm_c = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(dropout_b)\n",
    "    dropout_c = Dropout(0.2)(lstm_c)\n",
    "    lstm_d = LSTM(w2v_size, return_sequences=True)(dropout_c)\n",
    "\n",
    "    model = Model(inputs=model_input, outputs=lstm_d)\n",
    "    model.compile(optimizer=\"RMSprop\", loss='cosine_proximity')\n",
    "    model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    l2_in = Lambda(lambda x: K.l2_normalize(x, axis=2))(model_input)\n",
    "    l2_out = Lambda(lambda x: K.l2_normalize(x, axis=2))(lstm_d)\n",
    "    l2_multiply = Multiply()([l2_in,l2_out])\n",
    "    dot_product = Lambda(lambda x: -K.sum(x,axis=2))(l2_multiply)\n",
    "    output_max = Lambda(lambda x: K.max(x, axis=1, keepdims=True))(dot_product)\n",
    "    output_avg = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(dot_product)\n",
    "    \n",
    "    flat_in = Flatten()(model_input)\n",
    "    flat_out = Flatten()(lstm_d)\n",
    "    output_dot = Dot(axes=(1,1), normalize=True)([flat_in, flat_out])\n",
    "\n",
    "    max_pred_model = Model(inputs=model_input, outputs=output_max)\n",
    "    avg_pred_model = Model(inputs=model_input, outputs=output_avg)\n",
    "    dot_pred_model = Model(inputs=model_input, outputs=output_dot)\n",
    "    \n",
    "    dyad_max_preds = max_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    dyad_avg_preds = avg_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    dyad_dot_preds = dot_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    \n",
    "    pickle.dump([dyad_max_preds, dyad_avg_preds, dyad_dot_preds],\n",
    "                open(\"asa_sdss_final/dyad_\"+str(ii)+\".pickle\",\"wb\"))\n",
    "    \n",
    "    print(str(ii)+\" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_max_preds_list = []\n",
    "internal_avg_preds_list = []\n",
    "internal_dot_preds_list = []\n",
    "\n",
    "X_test, Y_test = cicids_processing(cicids_internal_hour_testing[\"sequence\"].tolist(),\n",
    "                         cicids_internal_hour_testing[\"label_sequence\"].tolist(),\n",
    "                         w2v_internal_hour, w2v_size, seq_len, seq_skip, False)\n",
    "pickle.dump([X_test, Y_test],open(\"asa_sdss_final/internal_truth.pickle\",\"wb\"))\n",
    "\n",
    "for ii in range(num_models):\n",
    "    \n",
    "    X_train, Y_train = cicids_processing(cicids_internal_hour_training[\"sequence\"].tolist(),\n",
    "                         cicids_internal_hour_training[\"label_sequence\"].tolist(),\n",
    "                         w2v_internal_hour, w2v_size, seq_len, seq_skip, True)\n",
    "    \n",
    "    model_input = Input(shape=(seq_len, w2v_size))\n",
    "    lstm_a = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(model_input)\n",
    "    lstm_b = Bidirectional(LSTM(lstm_b_size, return_sequences=False), merge_mode=\"concat\")(lstm_a)\n",
    "    dropout_a = Dropout(0.2)(lstm_b)\n",
    "    dense_layer = Dense(dense_size, activation=\"tanh\")(dropout_a)\n",
    "    repeat_layer = RepeatVector(seq_len)(dense_layer)\n",
    "    dropout_b = Dropout(0.2)(repeat_layer)\n",
    "    lstm_c = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(dropout_b)\n",
    "    dropout_c = Dropout(0.2)(lstm_c)\n",
    "    lstm_d = LSTM(w2v_size, return_sequences=True)(dropout_c)\n",
    "\n",
    "    model = Model(inputs=model_input, outputs=lstm_d)\n",
    "    model.compile(optimizer=\"RMSprop\", loss='cosine_proximity')\n",
    "    model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    l2_in = Lambda(lambda x: K.l2_normalize(x, axis=2))(model_input)\n",
    "    l2_out = Lambda(lambda x: K.l2_normalize(x, axis=2))(lstm_d)\n",
    "    l2_multiply = Multiply()([l2_in,l2_out])\n",
    "    dot_product = Lambda(lambda x: -K.sum(x,axis=2))(l2_multiply)\n",
    "    output_max = Lambda(lambda x: K.max(x, axis=1, keepdims=True))(dot_product)\n",
    "    output_avg = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(dot_product)\n",
    "    \n",
    "    flat_in = Flatten()(model_input)\n",
    "    flat_out = Flatten()(lstm_d)\n",
    "    output_dot = Dot(axes=(1,1), normalize=True)([flat_in, flat_out])\n",
    "\n",
    "    max_pred_model = Model(inputs=model_input, outputs=output_max)\n",
    "    avg_pred_model = Model(inputs=model_input, outputs=output_avg)\n",
    "    dot_pred_model = Model(inputs=model_input, outputs=output_dot)\n",
    "    \n",
    "    internal_max_preds = max_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    internal_avg_preds = avg_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    internal_dot_preds = dot_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    \n",
    "    pickle.dump([internal_max_preds, internal_avg_preds, internal_dot_preds],\n",
    "                open(\"asa_sdss_final/internal_\"+str(ii)+\".pickle\",\"wb\"))\n",
    "    \n",
    "    print(str(ii)+\" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_max_preds_list = []\n",
    "external_avg_preds_list = []\n",
    "external_dot_preds_list = []\n",
    "\n",
    "X_test, Y_test = cicids_processing(cicids_external_hour_testing[\"sequence\"].tolist(),\n",
    "                         cicids_external_hour_testing[\"label_sequence\"].tolist(),\n",
    "                         w2v_external_hour, w2v_size, seq_len, seq_skip, False)\n",
    "pickle.dump([X_test, Y_test],open(\"asa_sdss_final/external_truth.pickle\",\"wb\"))\n",
    "\n",
    "for ii in range(num_models):\n",
    "    \n",
    "    X_train, Y_train = cicids_processing(cicids_external_hour_training[\"sequence\"].tolist(),\n",
    "                         cicids_external_hour_training[\"label_sequence\"].tolist(),\n",
    "                         w2v_external_hour, w2v_size, seq_len, seq_skip, True)\n",
    "    \n",
    "    model_input = Input(shape=(seq_len, w2v_size))\n",
    "    lstm_a = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(model_input)\n",
    "    lstm_b = Bidirectional(LSTM(lstm_b_size, return_sequences=False), merge_mode=\"concat\")(lstm_a)\n",
    "    dropout_a = Dropout(0.2)(lstm_b)\n",
    "    dense_layer = Dense(dense_size, activation=\"tanh\")(dropout_a)\n",
    "    repeat_layer = RepeatVector(seq_len)(dense_layer)\n",
    "    dropout_b = Dropout(0.2)(repeat_layer)\n",
    "    lstm_c = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(dropout_b)\n",
    "    dropout_c = Dropout(0.2)(lstm_c)\n",
    "    lstm_d = LSTM(w2v_size, return_sequences=True)(dropout_c)\n",
    "\n",
    "    model = Model(inputs=model_input, outputs=lstm_d)\n",
    "    model.compile(optimizer=\"RMSprop\", loss='cosine_proximity')\n",
    "    model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    l2_in = Lambda(lambda x: K.l2_normalize(x, axis=2))(model_input)\n",
    "    l2_out = Lambda(lambda x: K.l2_normalize(x, axis=2))(lstm_d)\n",
    "    l2_multiply = Multiply()([l2_in,l2_out])\n",
    "    dot_product = Lambda(lambda x: -K.sum(x,axis=2))(l2_multiply)\n",
    "    output_max = Lambda(lambda x: K.max(x, axis=1, keepdims=True))(dot_product)\n",
    "    output_avg = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(dot_product)\n",
    "    \n",
    "    flat_in = Flatten()(model_input)\n",
    "    flat_out = Flatten()(lstm_d)\n",
    "    output_dot = Dot(axes=(1,1), normalize=True)([flat_in, flat_out])\n",
    "\n",
    "    max_pred_model = Model(inputs=model_input, outputs=output_max)\n",
    "    avg_pred_model = Model(inputs=model_input, outputs=output_avg)\n",
    "    dot_pred_model = Model(inputs=model_input, outputs=output_dot)\n",
    "    \n",
    "    external_max_preds = max_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    external_avg_preds = avg_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    external_dot_preds = dot_pred_model.predict(X_test, batch_size=batch_size)\n",
    "    \n",
    "    pickle.dump([external_max_preds, external_avg_preds, external_dot_preds],\n",
    "                open(\"asa_sdss_final/external_\"+str(ii)+\".pickle\",\"wb\"))\n",
    "    \n",
    "    print(str(ii)+\" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "def evaluate_results(Y_test, score_arrays):\n",
    "    models = {}\n",
    "    for val, key in enumerate(np.unique(Y_test).tolist()):\n",
    "        models[key] = {\"fpr\":[],\"tpr\":[],\"threshold\":[],\"auc\":[]}\n",
    "        for preds in score_arrays:\n",
    "            results_tuple = roc_curve(Y_test == key, preds if key!=\"BENIGN\" else -preds, pos_label=1)\n",
    "            models[key][\"fpr\"].append(results_tuple[0])\n",
    "            models[key][\"tpr\"].append(results_tuple[1])\n",
    "            models[key][\"threshold\"].append(results_tuple[2])\n",
    "            models[key][\"auc\"].append(auc(results_tuple[0], results_tuple[1]))\n",
    "    return models\n",
    "\n",
    "def make_roc_plots(result_set, attack_names, title_postfix=\"\", file_prefix=\"\"):\n",
    "    for key, value in attack_names:\n",
    "        \n",
    "        fpr = {i:v for i,v in enumerate(result_set[key][\"fpr\"])}\n",
    "        tpr = {i:v for i,v in enumerate(result_set[key][\"tpr\"])}\n",
    "        roc_auc = {i:v for i,v in enumerate(result_set[key][\"auc\"])}\n",
    "        n_classes = len(fpr)\n",
    "        \n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        # Plot all ROC curves\n",
    "        plt.figure()\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='Average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),\n",
    "                 color='black', linewidth=4)\n",
    "\n",
    "        colors = cycle(['gray'])\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=1)\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(value + \" ROC Curve\"+title_postfix)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig('asa_sdss_final_figs/'+file_prefix+key+\"_dot.pdf\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [(\"BENIGN\",\"All Attacks\"),\n",
    "           (\"Bot\",\"Bot\"),\n",
    "           (\"DDoS\",\"DDoS\"),\n",
    "           (\"DoS GoldenEye\",\"DoS GoldenEye\"),\n",
    "           (\"DoS Hulk\",\"DoS Hulk\"),\n",
    "           (\"DoS Slowhttptest\",\"DoS Slow HTTP Test\"),\n",
    "           (\"DoS slowloris\",\"DoS Slow Loris\"),\n",
    "           (\"FTPPatator\",\"FTPPatator\"),\n",
    "           (\"SSHPatator\",\"SSHPatator\"),\n",
    "           (\"Heartbleed\",\"Heartbleed\"),\n",
    "           (\"Infiltration\",\"Infiltration\"),\n",
    "           (\"PortScan\",\"Port Scan\"),\n",
    "           (\"Web Attack  Brute Force\",\"Web Attack Brute Force\"),\n",
    "           (\"Web Attack  Sql Injection\",\"Web Attack SQL Injection\"),\n",
    "           (\"Web Attack  XSS\",\"Web Attack XSS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_rules = [\"source\",\"destination\",\"dyad\",\"internal\",\"external\"]\n",
    "for agg in agg_rules:\n",
    "    dot_0 = pickle.load(open(\"asa_sdss_final/\"+agg+\"_0.pickle\",\"rb\"))[2]\n",
    "    dot_1 = pickle.load(open(\"asa_sdss_final/\"+agg+\"_1.pickle\",\"rb\"))[2]\n",
    "    dot_2 = pickle.load(open(\"asa_sdss_final/\"+agg+\"_2.pickle\",\"rb\"))[2]\n",
    "    Y_test = pickle.load(open(\"asa_sdss_final/\"+agg+\"_truth.pickle\",\"rb\"))[1]\n",
    "\n",
    "    models_dot = evaluate_results(Y_test, [dot_0, dot_1, dot_2])\n",
    "    make_roc_plots(models_dot, attacks, \" - \"+agg+\" aggregation\", \"agg_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
