{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Aggregation Rules for Anomaly Detection in Computer Network Traffic\n",
    "## Replication File 1 of 3\n",
    "\n",
    "Benjamin J. Radford, Bartley D. Richardson, and Shawn E. Davis\n",
    "\n",
    "Paper available: [arXiv:1805.03735v2](https://arxiv.org/abs/1805.03735).\n",
    "\n",
    "DISTRIBUTION STATEMENT A: Approved for public release. \n",
    "\n",
    "This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import dependencies\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.engine.topology import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.optimizers import TFOptimizer, RMSprop\n",
    "\n",
    "## Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set modeling parameters\n",
    "##\n",
    "\n",
    "seq_len = 10\n",
    "seq_skip = 3\n",
    "\n",
    "w2v_size = 25\n",
    "w2v_min_count = 3\n",
    "w2v_window = 10\n",
    "w2v_workers = 4\n",
    "\n",
    "embedding_a_size = 100\n",
    "lstm_a_size = 25\n",
    "lstm_b_size = 25\n",
    "dense_size = 100\n",
    "\n",
    "validation_split = 0.1\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "\n",
    "cicids_training = datetime.datetime.strptime(\"2017-07-04 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "num_models = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CICIDS2017 data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/anaconda3/envs/py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (20,21,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/anaconda3/envs/py2/lib/python2.7/site-packages/ipykernel_launcher.py:15: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Read in CICIDS2017 Dataset\n",
    "##\n",
    "\n",
    "## Set a working directory and point to your data\n",
    "wd = \"./\"\n",
    "cicids_files = \"data/CICIDS2017/*.csv\"\n",
    "\n",
    "## Load the data\n",
    "print(\"Reading CICIDS2017 data...\")\n",
    "files = glob.glob(wd+cicids_files)\n",
    "cicids_data = []\n",
    "for ff in files:\n",
    "    cicids_data.append(pandas.read_csv(ff, encoding=\"Latin1\"))\n",
    "cicids_data = pandas.concat(cicids_data)\n",
    "\n",
    "## Set CICIDS2017 internal IPs in case they're required later\n",
    "cicids_internal = set([\"192.168.10.50\", \"205.174.165.68\",\n",
    "            \"192.168.10.51\", \"205.174.165.66\", \"192.168.10.19\",\n",
    "            \"192.168.10.17\", \"192.168.10.16\",\n",
    "            \"192.168.10.12\", \"192.168.10.9\",\n",
    "            \"192.168.10.5\", \"192.168.10.8\",\n",
    "            \"192.168.10.14\", \"192.168.10.15\",\n",
    "            \"192.168.10.25\", \"205.174.165.80\", \n",
    "            \"172.16.0.1\",\"192.168.10.3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Define functions required to clean CICIDS2017\n",
    "##\n",
    "\n",
    "def cicids_fixdate(x):\n",
    "    \"\"\"Fix dates - This is specific to the CICIDS2017 dataset.\"\"\"\n",
    "    try:\n",
    "        d = datetime.datetime.strptime(x,\"%d/%m/%Y %H:%M:%S\")\n",
    "    except:\n",
    "        d = datetime.datetime.strptime(x,\"%d/%m/%Y %H:%M\")\n",
    "    return(d)\n",
    "\n",
    "def cicids_fixattacknames(x):\n",
    "    \"\"\"Fix attack names - This is specific to CICIDS2017 dataset.\"\"\"\n",
    "    return(re.sub('[^0-9a-zA-Z ]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing column names...\n",
      "BENIGN    713828\n",
      "DDoS       41835\n",
      "Name: Label, dtype: int64\n",
      "Dropping incomplete records...\n",
      "Fixing the timestamps...\n",
      "Fixing attack names...\n",
      "Generating floor(log2(bytes)) feature...\n",
      "Generate unique index values...\n",
      "Removing entries without an internal machine...\n",
      "Before remove ext-ext: 755663\n",
      "After remove ext-ext: 755384\n",
      "Generate internal and external host fields...\n",
      "Generating dyads...\n",
      "Generating hour bins...\n",
      "Generating protocol:port:floor(log(avg(bytes/packet))) feature (a.k.a. 'token_source' and 'token_destination')...\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Clean CICIDS2017\n",
    "## \n",
    "\n",
    "print(\"Fixing column names...\")\n",
    "cicids_data.columns = [a.lstrip() for a in cicids_data.columns]\n",
    "cicids_data = cicids_data[[\"Source Port\",\"Destination Port\",\"Source IP\",\"Destination IP\",\"Label\",\"Timestamp\",\"Protocol\",\"Average Packet Size\",\"Total Fwd Packets\",\"Total Backward Packets\"]]\n",
    "print(cicids_data.Label.value_counts())\n",
    "\n",
    "print(\"Dropping incomplete records...\")\n",
    "cicids_data = cicids_data.dropna(axis=0,how=\"all\")\n",
    "\n",
    "print(\"Fixing the timestamps...\")\n",
    "cicids_data[\"Timestamp\"] = cicids_data[\"Timestamp\"].apply(lambda x: cicids_fixdate(x))\n",
    "\n",
    "print(\"Fixing attack names...\")\n",
    "cicids_data[\"Label\"] = cicids_data[\"Label\"].apply(lambda x: cicids_fixattacknames(x))\n",
    "\n",
    "print(\"Generating floor(log2(bytes)) feature...\")\n",
    "cicids_data[\"logbytes\"] = cicids_data[[\"Average Packet Size\",\"Total Fwd Packets\",\"Total Backward Packets\"]].apply(lambda x: math.floor(math.log(float(x[0] * (x[1]+x[2]))+1.,2)),axis=1)\n",
    "\n",
    "print(\"Generate unique index values...\")\n",
    "cicids_data[\"index\"] = list(range(0,cicids_data.shape[0]))\n",
    "\n",
    "print(\"Removing entries without an internal machine...\")\n",
    "print(\"Before remove ext-ext: {}\".format(str(cicids_data.shape[0])))\n",
    "cicids_data = cicids_data.loc[(cicids_data[\"Source IP\"].isin(cicids_internal) | cicids_data[\"Destination IP\"].isin(cicids_internal))]\n",
    "print(\"After remove ext-ext: {}\".format(str(cicids_data.shape[0])))\n",
    "\n",
    "print(\"Generate internal and external host fields...\")\n",
    "cicids_data[\"internal\"] = cicids_data[[\"Source IP\",\"Destination IP\"]].apply(lambda x: x[0] if x[0] in cicids_internal else x[1], axis=1)\n",
    "cicids_data[\"external\"] = cicids_data[[\"Source IP\",\"Destination IP\"]].apply(lambda x: x[1] if x[0] in cicids_internal else x[0], axis=1)\n",
    "\n",
    "print(\"Generating dyads...\")\n",
    "cicids_data[\"dyad\"] = cicids_data[[\"Source IP\",\"Destination IP\"]].apply(lambda x: x[0]+\":\"+x[1], axis=1)\n",
    "\n",
    "print(\"Generating hour bins...\")\n",
    "cicids_data[\"hour\"] = cicids_data[\"Timestamp\"].apply(lambda x: str(x)[0:13])\n",
    "\n",
    "print(\"Generating protocol:port:floor(log(avg(bytes/packet))) feature (a.k.a. 'token_source' and 'token_destination')...\")\n",
    "cicids_data[\"protobytes\"] = cicids_data[[\"Protocol\",\"logbytes\"]].apply(lambda x: str(x[0])+\":\"+str(x[1]), axis=1)\n",
    "cicids_data[\"port\"] = cicids_data[[\"Source Port\",\"Destination Port\"]].apply(lambda x: \"port:\"+str(min(min(x[0],10000),min(x[1],10000))), axis=1)\n",
    "cicids_data[\"label\"] = cicids_data[\"Label\"].apply(lambda x: x)\n",
    "cicids_data[\"time\"] = cicids_data[\"Timestamp\"].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate naive token frequency models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate naive token frequency models...\")\n",
    "protobytes_train = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) < cicids_training].protobytes.tolist()\n",
    "protobytes_test = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) >= cicids_training].protobytes.tolist()\n",
    "ports_train = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) < cicids_training].port.tolist()\n",
    "ports_test = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) >= cicids_training].port.tolist()\n",
    "labels = cicids_data[pandas.to_datetime(cicids_data[\"time\"]) >= cicids_training].label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "protobytes_freq = Counter(protobytes_train)\n",
    "ports_freq = Counter(ports_train)\n",
    "protobytes_sum = float(sum([a for b,a in protobytes_freq.iteritems()]))\n",
    "ports_sum = float(sum([a for b,a in ports_freq.iteritems()]))\n",
    "protobytes_prob = {w:(f/protobytes_sum) for w,f in protobytes_freq.iteritems()}\n",
    "ports_prob = {w:(f/ports_sum) for w,f in ports_freq.iteritems()}\n",
    "protobytes_scores = np.asarray([protobytes_prob[a] if a in protobytes_prob else 0. for a in protobytes_test])\n",
    "ports_scores = np.asarray([ports_prob[a] if a in ports_prob else 0. for a in ports_test])\n",
    "pickle.dump(np.array(labels),open(\"results/freq_labels.pickle\",\"wb\"))\n",
    "pickle.dump(ports_scores,open(\"results/freq_ports.pickle\",\"wb\"))\n",
    "pickle.dump(protobytes_scores,open(\"results/freq_protobytes.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating forward and reverse dictionaries...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating forward and reverse dictionaries...\")\n",
    "protobytes_fwd_dict = {str(w):str(i+1) for i, w in enumerate(list(set(cicids_data[\"protobytes\"].unique().tolist())))}\n",
    "protobytes_fwd_dict[\"0\"] = 0\n",
    "protobytes_rev_dict = {i:w for w,i in protobytes_fwd_dict.iteritems()}\n",
    "port_fwd_dict = {str(w):str(i+1) for i, w in enumerate(list(set(cicids_data[\"port\"].unique().tolist())))}\n",
    "port_fwd_dict[\"0\"] = 0\n",
    "port_rev_dict = {i:w for w,i in protobytes_fwd_dict.iteritems()}\n",
    "\n",
    "cicids_data = cicids_data.replace({\"protobytes\": protobytes_fwd_dict, \"port\": port_fwd_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat token strings...\n",
      "Generate all sequences...\n",
      "Add training set indicator...\n",
      "Prepend sequences with zero values...\n",
      "Split into training and test sets...\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Generate sequences from CICIDS2017\n",
    "##\n",
    "\n",
    "## Concat token strings\n",
    "print(\"Concat token strings...\")\n",
    "cicids_source_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"Source IP\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "cicids_destination_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"Destination IP\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "cicids_dyad_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"dyad\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "cicids_internal_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"internal\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "cicids_external_hour = cicids_data.sort_values([\"Timestamp\"]).groupby([\"external\",\"hour\"])[[\"protobytes\",\"port\",\"label\",\"time\"]].agg(lambda x: \",\".join(x))\n",
    "\n",
    "## Generate sequences\n",
    "print(\"Generate all sequences...\")\n",
    "cicids_source_hour[\"protobytes_sequence\"] = cicids_source_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_destination_hour[\"protobytes_sequence\"] = cicids_destination_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_dyad_hour[\"protobytes_sequence\"] = cicids_dyad_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_internal_hour[\"protobytes_sequence\"] = cicids_internal_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_external_hour[\"protobytes_sequence\"] = cicids_external_hour[\"protobytes\"].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "cicids_source_hour[\"port_sequence\"] = cicids_source_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_destination_hour[\"port_sequence\"] = cicids_destination_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_dyad_hour[\"port_sequence\"] = cicids_dyad_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_internal_hour[\"port_sequence\"] = cicids_internal_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_external_hour[\"port_sequence\"] = cicids_external_hour[\"port\"].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "cicids_source_hour[\"label_sequence\"] = cicids_source_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_destination_hour[\"label_sequence\"] = cicids_destination_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_dyad_hour[\"label_sequence\"] = cicids_dyad_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_internal_hour[\"label_sequence\"] = cicids_internal_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_external_hour[\"label_sequence\"] = cicids_external_hour[\"label\"].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "cicids_source_hour[\"time_sequence\"] = cicids_source_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_destination_hour[\"time_sequence\"] = cicids_destination_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_dyad_hour[\"time_sequence\"] = cicids_dyad_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_internal_hour[\"time_sequence\"] = cicids_internal_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "cicids_external_hour[\"time_sequence\"] = cicids_external_hour[\"time\"].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "## Add training set indicator\n",
    "print(\"Add training set indicator...\")\n",
    "cicids_source_hour[\"training\"] = cicids_source_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "cicids_destination_hour[\"training\"] = cicids_destination_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "cicids_dyad_hour[\"training\"] = cicids_dyad_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "cicids_internal_hour[\"training\"] = cicids_internal_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "cicids_external_hour[\"training\"] = cicids_external_hour[\"time_sequence\"].apply(lambda x: max([datetime.datetime.strptime(a, \"%Y-%m-%d %H:%M:%S\") for a in x]) < cicids_training)\n",
    "\n",
    "## Prepend sequences to correct length\n",
    "print(\"Prepend sequences with zero values...\")\n",
    "cicids_source_hour[\"port_sequence\"] = cicids_source_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_destination_hour[\"port_sequence\"] = cicids_destination_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_dyad_hour[\"port_sequence\"] = cicids_dyad_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_internal_hour[\"port_sequence\"] = cicids_internal_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_external_hour[\"port_sequence\"] = cicids_external_hour[\"port_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "\n",
    "cicids_source_hour[\"protobytes_sequence\"] = cicids_source_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_destination_hour[\"protobytes_sequence\"] = cicids_destination_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_dyad_hour[\"protobytes_sequence\"] = cicids_dyad_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_internal_hour[\"protobytes_sequence\"] = cicids_internal_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_external_hour[\"protobytes_sequence\"] = cicids_external_hour[\"protobytes_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "\n",
    "cicids_source_hour[\"label_sequence\"] = cicids_source_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_destination_hour[\"label_sequence\"] = cicids_destination_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_dyad_hour[\"label_sequence\"] = cicids_dyad_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_internal_hour[\"label_sequence\"] = cicids_internal_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_external_hour[\"label_sequence\"] = cicids_external_hour[\"label_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "\n",
    "cicids_source_hour[\"time_sequence\"] = cicids_source_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_destination_hour[\"time_sequence\"] = cicids_destination_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_dyad_hour[\"time_sequence\"] = cicids_dyad_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_internal_hour[\"time_sequence\"] = cicids_internal_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "cicids_external_hour[\"time_sequence\"] = cicids_external_hour[\"time_sequence\"].apply(lambda x: [0]*(seq_len - 1) + x)\n",
    "\n",
    "## Split into training and test sets\n",
    "print(\"Split into training and test sets...\")\n",
    "cicids_source_hour_training = cicids_source_hour[cicids_source_hour[\"training\"]==True]\n",
    "cicids_destination_hour_training = cicids_destination_hour[cicids_destination_hour[\"training\"]==True]\n",
    "cicids_dyad_hour_training = cicids_dyad_hour[cicids_dyad_hour[\"training\"]==True]\n",
    "cicids_internal_hour_training = cicids_internal_hour[cicids_internal_hour[\"training\"]==True]\n",
    "cicids_external_hour_training = cicids_external_hour[cicids_external_hour[\"training\"]==True]\n",
    "\n",
    "cicids_source_hour_testing = cicids_source_hour[cicids_source_hour[\"training\"]==False]\n",
    "cicids_destination_hour_testing = cicids_destination_hour[cicids_destination_hour[\"training\"]==False]\n",
    "cicids_dyad_hour_testing = cicids_dyad_hour[cicids_dyad_hour[\"training\"]==False]\n",
    "cicids_internal_hour_testing = cicids_internal_hour[cicids_internal_hour[\"training\"]==False]\n",
    "cicids_external_hour_testing = cicids_external_hour[cicids_external_hour[\"training\"]==False]\n",
    "\n",
    "# def subToken(corpus, count):\n",
    "#     counts = Counter([a for sublist in corpus for a in sublist])\n",
    "#     above_count = set([key for key,val in counts.iteritems() if val >= count])\n",
    "#     return [map(lambda x: x if x in above_count else \"UNK\", a) for a in corpus]\n",
    "\n",
    "# ## Word2Vec models for sequence tokens\n",
    "# w2v_source_hour = Word2Vec(subToken(cicids_source_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n",
    "# w2v_destination_hour = Word2Vec(subToken(cicids_destination_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n",
    "# w2v_dyad_hour = Word2Vec(subToken(cicids_dyad_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n",
    "# w2v_internal_hour = Word2Vec(subToken(cicids_internal_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n",
    "# w2v_external_hour = Word2Vec(subToken(cicids_external_hour_training[\"sequence\"].tolist(), w2v_min_count), min_count=w2v_min_count, size=w2v_size, window=w2v_window, workers=w2v_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save stuff...\n"
     ]
    }
   ],
   "source": [
    "print(\"Save stuff...\")\n",
    "pickle.dump(port_fwd_dict,open(\"data/port_fwd_dict.pickle\",\"wb\"))\n",
    "pickle.dump(port_rev_dict,open(\"data/port_rev_dict.pickle\",\"wb\"))\n",
    "pickle.dump(protobytes_fwd_dict,open(\"data/protobytes_fwd_dict.pickle\",\"wb\"))\n",
    "pickle.dump(protobytes_rev_dict,open(\"data/protobytes_rev_dict.pickle\",\"wb\"))\n",
    "cicids_source_hour_training.to_pickle(\"data/cicids_source_hour_training.pickle\")\n",
    "cicids_destination_hour_training.to_pickle(\"data/cicids_destination_hour_training.pickle\")\n",
    "cicids_dyad_hour_training.to_pickle(\"data/cicids_dyad_hour_training.pickle\")\n",
    "cicids_internal_hour_training.to_pickle(\"data/cicids_internal_hour_training.pickle\")\n",
    "cicids_external_hour_training.to_pickle(\"data/cicids_external_hour_training.pickle\")\n",
    "cicids_source_hour_testing.to_pickle(\"data/cicids_source_hour_testing.pickle\")\n",
    "cicids_destination_hour_testing.to_pickle(\"data/cicids_destination_hour_testing.pickle\")\n",
    "cicids_dyad_hour_testing.to_pickle(\"data/cicids_dyad_hour_testing.pickle\")\n",
    "cicids_internal_hour_testing.to_pickle(\"data/cicids_internal_hour_testing.pickle\")\n",
    "cicids_external_hour_testing.to_pickle(\"data/cicids_external_hour_testing.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
