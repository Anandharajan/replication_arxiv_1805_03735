{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Aggregation Rules for Anomaly Detection in Computer Network Traffic\n",
    "## Replication File 2 of 3\n",
    "\n",
    "Benjamin J. Radford, Bartley D. Richardson, and Shawn E. Davis\n",
    "\n",
    "Paper available: [arXiv:1805.03735v2](https://arxiv.org/abs/1805.03735).\n",
    "\n",
    "DISTRIBUTION STATEMENT A: Approved for public release. \n",
    "\n",
    "This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss, auc, roc_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.engine.topology import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.optimizers import TFOptimizer, RMSprop\n",
    "\n",
    "## Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Set modeling parameters\n",
    "##\n",
    "\n",
    "seq_len = 10\n",
    "seq_skip = 1\n",
    "\n",
    "w2v_size = 25\n",
    "w2v_min_count = 3\n",
    "w2v_window = 10\n",
    "w2v_workers = 4\n",
    "\n",
    "embedding_a_size = 100\n",
    "lstm_a_size = 25\n",
    "lstm_b_size = 25\n",
    "dense_size = 100\n",
    "\n",
    "validation_split = 0.1\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "\n",
    "cicids_training = datetime.datetime.strptime(\"2017-07-04 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "num_models = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load global stuff...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load global stuff...\")\n",
    "port_fwd_dict = pickle.load(open(\"data/port_fwd_dict.pickle\",\"rb\"))\n",
    "port_rev_dict = pickle.load(open(\"data/port_rev_dict.pickle\",\"rb\"))\n",
    "protobytes_fwd_dict = pickle.load(open(\"data/protobytes_fwd_dict.pickle\",\"rb\"))\n",
    "protobytes_rev_dict = pickle.load(open(\"data/protobytes_rev_dict.pickle\",\"rb\"))\n",
    "\n",
    "# pickle.dump(port_fwd_dict,open(\"data/port_fwd_dict.pickle\",\"wb\"))\n",
    "# pickle.dump(port_rev_dict,open(\"data/port_rev_dict.pickle\",\"wb\"))\n",
    "# pickle.dump(protobytes_fwd_dict,open(\"data/protobytes_fwd_dict.pickle\",\"wb\"))\n",
    "# pickle.dump(protobytes_rev_dict,open(\"data/protobytes_rev_dict.pickle\",\"wb\"))\n",
    "# cicids_source_hour_training.to_pickle(\"data/cicids_source_hour_training.pickle\")\n",
    "# cicids_destination_hour_training.to_pickle(\"data/cicids_destination_hour_training.pickle\")\n",
    "# cicids_dyad_hour_training.to_pickle(\"data/cicids_dyad_hour_training.pickle\")\n",
    "# cicids_internal_hour_training.to_pickle(\"data/cicids_internal_hour_training.pickle\")\n",
    "# cicids_external_hour_training.to_pickle(\"data/cicids_external_hour_training.pickle\")\n",
    "# cicids_source_hour_testing.to_pickle(\"data/cicids_source_hour_testing.pickle\")\n",
    "# cicids_destination_hour_testing.to_pickle(\"data/cicids_destination_hour_testing.pickle\")\n",
    "# cicids_dyad_hour_testing.to_pickle(\"data/cicids_dyad_hour_testing.pickle\")\n",
    "# cicids_internal_hour_testing.to_pickle(\"data/cicids_internal_hour_testing.pickle\")\n",
    "# cicids_external_hour_testing.to_pickle(\"data/cicids_external_hour_testing.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_mode(X):\n",
    "    X = filter(lambda x: x != \"X\", X)\n",
    "    if len(set(X)) == 1 and list(set(X))[0] == \"BENIGN\":\n",
    "        return(\"BENIGN\")\n",
    "    else:\n",
    "        X = [a for a in X if a != \"BENIGN\"]\n",
    "        return(max(set(X), key=X.count))\n",
    "    \n",
    "def cicids_processing(sequences, labels, dict_size, seq_len, seq_skip, resample=False):\n",
    "    X = []\n",
    "    Y = []\n",
    "    L = []\n",
    "    for ii, token_seq in enumerate(sequences):\n",
    "        label_seq = labels[ii]\n",
    "        for jj in range(0, len(token_seq)-seq_len, seq_skip):\n",
    "            X.append(token_seq[jj:(jj+seq_len)])\n",
    "            Y.append(to_categorical(int(token_seq[jj+seq_len])-1, dict_size))\n",
    "            L.append(label_seq[jj+seq_len])\n",
    "            \n",
    "    if resample==True:\n",
    "        indices = np.random.choice(np.arange(len(X)),size=len(X),replace=True)\n",
    "    else:\n",
    "        indices = np.arange(len(X))\n",
    "    return(np.array(X)[indices], np.array(Y)[indices], np.array(L)[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 461187 samples, validate on 51243 samples\n",
      "Epoch 1/10\n",
      "366592/461187 [======================>.......] - ETA: 28s - loss: 1.6249"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9ef083f99ed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTFOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLazyAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlabel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport_fwd_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aggregations = [\"source\",\"destination\",\"dyad\",\"internal\",\"external\"]\n",
    "# aggregations=[\"external\"]\n",
    "\n",
    "for agg in aggregations:\n",
    "\n",
    "    cicids_testing = pickle.load(open(\"data/cicids_\"+agg+\"_hour_testing.pickle\",\"rb\"))\n",
    "    cicids_training = pickle.load(open(\"data/cicids_\"+agg+\"_hour_training.pickle\",\"rb\"))\n",
    "\n",
    "    X_test, Y_test, L_test = cicids_processing(cicids_testing[\"port_sequence\"].tolist(),\n",
    "                             cicids_testing[\"label_sequence\"].tolist(),\n",
    "                             len(port_fwd_dict)-1, seq_len, 3, False)\n",
    "\n",
    "#     pickle.dump(X_test, open(\"results/\"+agg+\"_port_truth_X.pickle\",\"wb\"))\n",
    "#     pickle.dump(Y_test, open(\"results/\"+agg+\"_port_truth_Y.pickle\",\"wb\"))\n",
    "    pickle.dump(L_test, open(\"results/\"+agg+\"_port_truth_L.pickle\",\"wb\"))\n",
    "\n",
    "    for ii in range(num_models):\n",
    "\n",
    "        X_train, Y_train, L_test = cicids_processing(cicids_training[\"port_sequence\"].tolist(),\n",
    "                             cicids_training[\"label_sequence\"].tolist(),\n",
    "                             len(port_fwd_dict)-1, seq_len, 1, True)\n",
    "\n",
    "        model_input = Input(shape=(seq_len, ))\n",
    "        embedding_a = Embedding(len(port_fwd_dict), 50, input_length=seq_len, mask_zero=True)(model_input)\n",
    "        lstm_a = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(embedding_a)\n",
    "        dropout_a = Dropout(0.2)(lstm_a)\n",
    "        lstm_b = Bidirectional(LSTM(lstm_b_size, return_sequences=False, activation=\"relu\"), merge_mode=\"concat\")(dropout_a)\n",
    "        dropout_b = Dropout(0.2)(lstm_b)\n",
    "        dense_layer = Dense(dense_size, activation=\"linear\")(dropout_b)\n",
    "        dropout_c = Dropout(0.2)(dense_layer)\n",
    "        model_output = Dense(len(port_fwd_dict)-1, activation=\"softmax\")(dropout_c)\n",
    "\n",
    "        model = Model(inputs=model_input, outputs=model_output)\n",
    "        model.compile(optimizer=TFOptimizer(tf.contrib.opt.LazyAdamOptimizer()), loss='categorical_crossentropy')\n",
    "\n",
    "        model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, class_weight = 'auto')\n",
    "\n",
    "        label_input = Input(shape=(len(port_fwd_dict)-1,))\n",
    "        score_output = Dot(axes=(1,1))([model_output, label_input])\n",
    "        pred_model = Model(inputs=[model_input,label_input], outputs=score_output)\n",
    "        preds = pred_model.predict([X_test,Y_test], batch_size=batch_size)\n",
    "\n",
    "        pickle.dump(preds,open(\"results/\"+agg+\"_\"+str(ii)+\"_port_preds.pickle\",\"wb\"))\n",
    "\n",
    "        print(agg + \" \" + str(ii) + \" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = [\"source\",\"destination\",\"dyad\",\"internal\",\"external\"]\n",
    "\n",
    "for agg in aggregations:\n",
    "\n",
    "    cicids_testing = pickle.load(open(\"data/cicids_\"+agg+\"_hour_testing.pickle\",\"rb\"))\n",
    "    cicids_training = pickle.load(open(\"data/cicids_\"+agg+\"_hour_training.pickle\",\"rb\"))\n",
    "\n",
    "    X_test, Y_test, L_test = cicids_processing(cicids_testing[\"protobytes_sequence\"].tolist(),\n",
    "                             cicids_testing[\"label_sequence\"].tolist(),\n",
    "                             len(protobytes_fwd_dict)-1, seq_len, seq_skip, False)\n",
    "\n",
    "#     pickle.dump(X_test, open(\"results/\"+agg+\"_protobytes_truth_X.pickle\",\"wb\"))\n",
    "#     pickle.dump(Y_test, open(\"results/\"+agg+\"_protobytes_truth_Y.pickle\",\"wb\"))\n",
    "    pickle.dump(L_test, open(\"results/\"+agg+\"_protobytes_truth_L.pickle\",\"wb\"))\n",
    "\n",
    "    for ii in range(num_models):\n",
    "\n",
    "        X_train, Y_train, L_test = cicids_processing(cicids_training[\"protobytes_sequence\"].tolist(),\n",
    "                             cicids_training[\"label_sequence\"].tolist(),\n",
    "                             len(protobytes_fwd_dict)-1, seq_len, seq_skip, True)\n",
    "\n",
    "        model_input = Input(shape=(seq_len, ))\n",
    "        embedding_a = Embedding(len(protobytes_fwd_dict), 50, input_length=seq_len, mask_zero=True)(model_input)\n",
    "        lstm_a = Bidirectional(LSTM(lstm_a_size, return_sequences=True), merge_mode=\"concat\")(embedding_a)\n",
    "        dropout_a = Dropout(0.2)(lstm_a)\n",
    "        lstm_b = Bidirectional(LSTM(lstm_b_size, return_sequences=False, activation=\"relu\"), merge_mode=\"concat\")(dropout_a)\n",
    "        dropout_b = Dropout(0.2)(lstm_b)\n",
    "        dense_layer = Dense(dense_size, activation=\"linear\")(dropout_b)\n",
    "        dropout_c = Dropout(0.2)(dense_layer)\n",
    "        model_output = Dense(len(protobytes_fwd_dict)-1, activation=\"softmax\")(dropout_c)\n",
    "\n",
    "        model = Model(inputs=model_input, outputs=model_output)\n",
    "        model.compile(optimizer=TFOptimizer(tf.contrib.opt.LazyAdamOptimizer()), loss='categorical_crossentropy')\n",
    "\n",
    "        model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, class_weight = 'auto')\n",
    "\n",
    "        label_input = Input(shape=(len(protobytes_fwd_dict)-1,))\n",
    "        score_output = Dot(axes=(1,1))([model_output, label_input])\n",
    "        pred_model = Model(inputs=[model_input,label_input], outputs=score_output)\n",
    "        preds = pred_model.predict([X_test,Y_test], batch_size=batch_size)\n",
    "\n",
    "        pickle.dump(preds,open(\"results/\"+agg+\"_\"+str(ii)+\"_protobytes_preds.pickle\",\"wb\"))\n",
    "\n",
    "        print(agg + \" \" + str(ii) + \" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
